{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 39. Methodenseminar\n",
    "## Big Data Module II: Introduction to Social Network Science with Python\n",
    "# 3.2 Scale-Free Networks (Exercise)\n",
    "**Author**: <a href='https://www.gesis.org/person/haiko.lietz'>Haiko Lietz</a>, GESIS - Leibniz Institute for the Social Sciences\n",
    "\n",
    "**Date**: 17 July 2019\n",
    "\n",
    "**Library versions**: ``networkx`` 2.2 ([documentation](https://networkx.github.io/documentation/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "In the demo, in section 3.2.4.1, we found preferential attachment to be linear for how citations in 2007-2009 predict citations in 1020-2012.\n",
    "#### How does preferential attachment evolve over time?\n",
    "In other words, does the Matthew Effect increase over time? Write a ``for`` loop. Compare these cultural dynamics to the emergence of the Small-World social (co-authorship) network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = pd.read_csv('../data/sns/citations.txt', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "references = pd.read_csv('../data/sns/references.txt', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "cited_references = pd.merge(left=citations, right=references, on='reference_id')\n",
    "publications = pd.read_csv('../data/sns/publications.txt', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "publications['time'] = (3*np.floor(publications['time']/3)+2).astype('int')\n",
    "cited_references_time = pd.merge(left=cited_references, right=publications[['publication_id', 'time']], on='publication_id')\n",
    "cited_references_time = cited_references_time.groupby(['time', 'reference']).size().reset_index(name='citations')\n",
    "cited_references_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(cited_references_time['time'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_reg(a):\n",
    "    # log and reshape data\n",
    "    x_log10 = np.log10(a[:, 0])\n",
    "    x_log10_reshape = x_log10.reshape(len(x_log10), 1)\n",
    "    y_log10 = np.log10(a[:, 1])\n",
    "    y_log10_reshape = y_log10.reshape(len(y_log10), 1)\n",
    "    # fit linear model in log space\n",
    "    import sklearn.linear_model as sk_lm\n",
    "    reg = sk_lm.LinearRegression()\n",
    "    reg.fit(x_log10_reshape, y_log10_reshape)\n",
    "    y_log10_reshape_predict = reg.predict(x_log10_reshape)\n",
    "    # create output\n",
    "    x_min = min(a[:, 0])\n",
    "    x_max = max(a[:, 0])\n",
    "    d = 10**reg.intercept_[0]\n",
    "    beta = reg.coef_[0][0]\n",
    "    from sklearn.metrics import r2_score\n",
    "    r2 = r2_score(y_log10_reshape, y_log10_reshape_predict)\n",
    "    a_fit = np.array([[x_min, d*x_min**beta], [x_max, d*x_max**beta]])\n",
    "    return beta, r2, a_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emergence of the co-authorship Small World for time slices\n",
    "<img src='images/social_emergence.png'>\n",
    "\n",
    "#### Emergence of the co-authorship Small World for cumulative time increases\n",
    "<img src='images/social_emergence_cum.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Four distributions can be extracted from the BTW13 dataset of political tweets:\n",
    "- number of followee selections\n",
    "- number of hashtag selections\n",
    "- number of mentionee selections\n",
    "- number of retweetee selections\n",
    "\n",
    "#### What functions best fit these distributions? How plausible are power law fits?\n",
    "Be careful with setting lower cutoffs ``xmin`` because they don't make sense if the best fit is not a power law. Plot the distribution first to decide what ``xmin`` to set. When comparing functions, disregard the ``lognormal`` (not the ``lognormal_positive``) as the lognormal can have a negative mean. Think about possible underlying mechanisms and what the patterns mean for the Twitter users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import powerlaw as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdf(l):\n",
    "    fit = pl.Fit(l, discrete=True, xmin=1)\n",
    "    fit.plot_pdf(marker='o', ls='', linear_bins=True)\n",
    "    fit.plot_pdf(marker='o', ls='', linear_bins=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_functions(f):\n",
    "    function = ['exponential', 'stretched_exponential', 'lognormal', 'lognormal_positive', 'power_law', 'truncated_power_law']\n",
    "    from numpy import zeros\n",
    "    f_compare_R = zeros((6, 6), dtype=float)\n",
    "    f_compare_p_R = zeros((6, 6), dtype=float)\n",
    "    for i in range(0, 6):\n",
    "        for j in range(0, 6):\n",
    "            R, p_R = f.distribution_compare(function[i], function[j])\n",
    "            f_compare_R[i, j] = R\n",
    "            f_compare_p_R[i, j] = p_R\n",
    "    from pandas import DataFrame\n",
    "    return DataFrame(f_compare_R, index=function, columns=function), DataFrame(f_compare_p_R, index=function, columns=function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_value(f, sims=2500):\n",
    "    prob = f.n_tail/len(f.data_original)\n",
    "    body = [x for x in f.data_original if x < f.xmin]\n",
    "    l = []\n",
    "    from random import random, sample\n",
    "    from powerlaw import Fit, Power_Law\n",
    "    for i in range(0, sims):\n",
    "        x = []\n",
    "        for j in range(0, len(f.data_original)):\n",
    "            if random() <= prob:\n",
    "                x.append(int(Power_Law(discrete=True, xmin=f.xmin, parameters=[f.power_law.alpha]).generate_random(1)))\n",
    "            else:\n",
    "                x.append(sample(body, 1)[0])\n",
    "        x_fit = Fit(x, discrete=True).power_law\n",
    "        l.append(x_fit.KS() > f.power_law.KS())\n",
    "    p = sum(l)/sims\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follow = pd.read_csv('../data/btw13/follow.txt', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "follow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_followers = list(follow.groupby('user_id_followee').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf(number_of_followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = pd.read_csv('../data/btw13/tag.txt', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "tag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_tags = list(tag.groupby('hashtag_id').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mentioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mention = pd.read_csv('../data/btw13/mention.txt', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "mention.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_mentioners = list(mention.groupby('user_id_mentionee').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf(number_of_mentioners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retweeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweet = pd.read_csv('../data/btw13/retweet.txt', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "retweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_retweeters = list(retweet.groupby('user_id_retweetee').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdf(number_of_retweeters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "The goal of Network Science is to find patterns that universally appear in complex systems (not only social systems). Ten complex networks, described [here](http://www.networksciencebook.com/translations/en/resources/data.html), are studied by Barabási in his book *Network Science*.\n",
    "#### What's the evidence for scale-freeness in the networks selected by Barabási?\n",
    "Can you reproduce the results [here](http://www.networksciencebook.com/chapter/4#advanced-c) (table 4.1)? Networks are ordered by size, so the collaboration network is fastest to assess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Barabási's Datsets\n",
    "Of these, the citation, email, metabolic, phonecalls, and www are directed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collaboration = pd.read_csv('../data/networksciencebook/collaboration.edgelist.zip', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "phonecalls = pd.read_csv('../data/networksciencebook/phonecalls.edgelist.zip', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "email = pd.read_csv('../data/networksciencebook/email.edgelist.zip', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "www = pd.read_csv('../data/networksciencebook/www.edgelist.zip', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "citation = pd.read_csv('../data/networksciencebook/citation.edgelist.zip', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "actor = pd.read_csv('../data/networksciencebook/actor.edgelist.zip', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "#internet = pd.read_csv('../data/networksciencebook/internet.edgelist.zip', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "#metabolic = pd.read_csv('../data/networksciencebook/metabolic.edgelist.zip', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "#powergrid = pd.read_csv('../data/networksciencebook/powergrid.edgelist.zip', header='infer', delimiter='\\t', encoding='utf-8')\n",
    "#protein = pd.read_csv('../data/networksciencebook/protein.edgelist.zip', header='infer', delimiter='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_power_law(l, xmin=None):\n",
    "    fit = pl.Fit(l, discrete=True, xmin=xmin)\n",
    "    fit.plot_pdf(marker='o', ls='', linear_bins=True)\n",
    "    fit.plot_pdf(marker='o', ls='')\n",
    "    #fit.exponential.plot_pdf(label='Exponential')\n",
    "    fit.stretched_exponential.plot_pdf(label='Stretched Exponential')\n",
    "    fit.lognormal_positive.plot_pdf(label='Lognormal')\n",
    "    fit.power_law.plot_pdf(label='Power Law')\n",
    "    fit.truncated_power_law.plot_pdf(label='Truncated Power Law')\n",
    "    plt.legend()\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Collaboration Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Co = nx.Graph(name='collaboration')\n",
    "Co.add_edges_from(collaboration.values)\n",
    "Co_degree = [degree for (node, degree) in Co.degree]\n",
    "print(nx.info(Co))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Phonecalls Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = nx.DiGraph(name='phonecalls')\n",
    "P.add_edges_from(phonecalls.values)\n",
    "P_out_degree = [out_degree for (node, out_degree) in P.out_degree if out_degree > 0]\n",
    "P_in_degree = [in_degree for (node, in_degree) in P.in_degree if in_degree > 0]\n",
    "print(nx.info(P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Email Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = nx.DiGraph(name='email')\n",
    "E.add_edges_from(email.values)\n",
    "E_out_degree = [out_degree for (node, out_degree) in E.out_degree if out_degree > 0]\n",
    "E_in_degree = [in_degree for (node, in_degree) in E.in_degree if in_degree > 0]\n",
    "print(nx.info(E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The WWW Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = nx.DiGraph(name='www')\n",
    "W.add_edges_from(www.values) # takes a few minutes\n",
    "W_out_degree = [out_degree for (node, out_degree) in W.out_degree if out_degree > 0]\n",
    "W_in_degree = [in_degree for (node, in_degree) in W.in_degree if in_degree > 0]\n",
    "print(nx.info(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Citation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ci = nx.DiGraph(name='citation')\n",
    "Ci.add_edges_from(citation.values) # takes a few minutes\n",
    "Ci_out_degree = [out_degree for (node, out_degree) in Ci.out_degree if out_degree > 0]\n",
    "Ci_in_degree = [in_degree for (node, in_degree) in Ci.in_degree if in_degree > 0]\n",
    "print(nx.info(Ci))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Actor Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = nx.Graph(name='actor')\n",
    "A.add_edges_from(actor.values) # takes a few minutes\n",
    "A_degree = [degree for (node, degree) in A.degree]\n",
    "print(nx.info(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
